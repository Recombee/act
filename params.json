{"name":"ACT","tagline":"","body":"## Aerospike Certification Tool (ACT)\r\n\r\nThis project is maintained by [Aerospike](http://www.aerospike.com)\r\n\r\n### Overview\r\n------------\r\n\r\nACT is a program for testing and certifying flash/SSD devices' performance for\r\nAerospike Database (with SATA, SAS and PCIe connectors).  ACT shows latency responses when you are reading from and writing to\r\nthe database concurrently while modeling the Aerospike Database server's I/O \r\npattern as closely as practical.\r\n\r\nACT allows you to test a single drive or multiple drives, using your actual connector/controller hardware.\r\n\r\nThe purpose of this certification is:\r\n\r\n1. Determine if an SSD device(s) will stand up to the demands of a high-speed real-time database\r\n2. Evaluate the upper limits of the throughput you can expect from a drive(s)\r\n\r\nNot all SSDs can handle the high volume of transactions required by high \r\nperformance real-time databases like Aerospike Database.  Many SSDs are rated \r\nfor 100K+ reads/writes per second, but in production the actual load they \r\ncan withstand for sustained periods of time is generally much lower.  In the process\r\nof testing many common SSDs in high-throughput tests, Aerospike developed this certification tool, ACT, that you can use to test/certify an \r\nSSD for yourself.\r\n\r\nWe have found performance – especially latency – of SSDs to be highly \r\ndependent on the write load the SSD is subjected to. Over the first few hours of a test, \r\nperformance can be excellent, but past the 4- to 10-hour mark (depending \r\non the drive), performance can suffer.\r\n\r\nThe ACT tool allows you to test an SSD device(s) for yourself.\r\nIn addition, Aerospike has tested a variety of SSDs and has specific recommendations.\r\nFor more information, visit the Aerospike Database documentation at:  https://docs.aerospike.com/.\r\n\r\n#### What the ACT Tool Does\r\n---------------------------\r\n\r\nACT performs a combination of large (128K) block reads and writes and small (1.5K) block reads, simulating\r\nstandard real-time database read/write loads.\r\n\r\nReads and write latency is measured for a long enough period of time (typically 24 hours) to evaluate drive stability and \r\noverall performance.\r\n\r\n**Traffic/Loading** \r\n\r\nYou can simulate:\r\n\r\n* 1x - normal load (2000 reads/sec and 1000 writes/sec per drive)\r\n* 3x - high load (6000 reads/sec and 3000 writes/sec per drive)\r\n* any other stress load or high-performance load (custom configurable)\r\n\r\n**Latency Rate Analysis**  \r\n\r\nACT's output shows latency rates broken down by intervals of 1, 8 and 64 ms (configurable). \r\n\r\nFor example, the test might indicate that 0.25% of requests\r\nfailed to complete in 1 ms or less and 0.01% of requests failed to complete in 8 ms or less.\r\n\r\n**Methodology**\r\n\r\nThe small read operations model client transaction requests.  The operations occur at a\r\nspecified rate.  Requests are added at this rate to a specified number of\r\nread transaction queues, each of which is serviced by a specified number of\r\nthreads.\r\n\r\nThe large-block read and write operations model the Aerospike server's\r\ndefragmentation process.  The operations occur at a specified rate, executed from one\r\ndedicated large-block read thread and one dedicated large-block write thread per\r\ndevice.\r\n\r\n#### Process for Certifying a Drive(s) for 3x Performance\r\n----------------------------------\r\n\r\n**In general, we recommend that you certify a drive for 3x performance.  Many drives do not pass the 3x\r\ncertification.  If you do not have a high-volume application, you may find that a 2x or 2.5x certification\r\nwill be sufficient.  The instructions below describe the 3x certification process, but you may need to adjust\r\nthe test based on your requirements.**\r\n\r\nTo certify a drive(s) for 3x performance with Aerospike Database requires two stages:\r\n\r\n1. Test a single drive to determine performance using the hardware configuration and connectors. The single-device certification will help you determine individual drive performance. \r\n2. If you will be using multiple drives, you can then run ACT to test multiple drives to see how \r\nthe results will be affected by the capacity of the bus or the throughput of the RAID controller that is managing your drives.\r\n\r\nThe test process with ACT is the same for both stages, but in the first stage you are testing a drive and\r\nin the second stage, you are testing the linearity/scalability of your connector with multiple drives installed.\r\n\r\nThe single-drive stage takes 48 hours.  The multi-drive stage takes an additional 48 hours.\r\n\r\n##### The first stage is to certify a single drive, to test the drive itself and the connection.\r\n\r\nBegin by installing your SSD device.  Our website has more details about installing SSDs in different environments\r\nand configurations at https://docs.aerospike.com/.\r\n\r\n**Test 1: Test under high loads**\r\n\r\nRun ACT for 24 hrs using the 3x test (6000 reads/sec and 3000 writes/sec).\r\nThe drive passes this test if less than 5% of operations fail to complete in 1 ms or less.\r\n\r\nMany drives fail the 3x test and are unsuitable for use with Aerospike Database.\r\n\r\n**Test 2: Stress test to ensure the drive does not fail under excessive loads**\r\n\r\nRun a 6x test for 24 hrs (12000 reads/sec and 6000 writes/sec).\r\nThe drive passes this test if ACT runs to completion, regardless of the error rate.\r\n\r\n**If you are testing a single drive, then the drive is certified when it passes Test 1 and Test 2.**\r\n\r\n##### The second stage is to certify multiple drives, to make sure that performance scales linearly when you add drives.\r\n\r\nInstall the additional SSDs to be tested.  Our website has more details about installing SSDs in different environments\r\nand configurations at https://docs.aerospike.com/.\r\n\r\n**Test 3: Repeat Test 1, with all drives installed: Test under high loads**\r\n\r\nRun ACT for 24 hrs using the 3x test (6000 reads/sec and 3000 writes/sec).\r\nThe drives pass this test if less than 5% of operations fail to complete in 1 ms or less.\r\n\r\n**Test 4: Repeat Test 2, with all drives installed: Stress test to ensure the drives do not fail under excessive loads**\r\n\r\nRun a 6x test for 24 hrs (12000 reads/sec and 6000 writes/sec).  The drives pass this test if ACT runs to completion, regardless of the error rate.\r\n\r\n**The drives are certified if they pass Test 3 and Test 4.**  Once the drive(s) has been certified, the drive can be used with Aerospike Database.\r\n\r\n&nbsp;\r\n\r\n#### Determining Expected Performance at Higher Throughput\r\n-------------------------------------------------------\r\n\r\nIf your application is going to have high volumes of transactions and your drive(s) passes the 3x certification, \r\nwe recommend that you test your drive to determine its upper limit on transaction processing latency.  This will help\r\nyou determine how many SSDs you will need to run your application when you are fully scaled up.\r\n\r\nTo certify a drive(s) at higher levels of performance, do the certification process as described above, but use higher loads (12x, 24x, etc.).\r\nTest the drive(s) at progressively higher rates until more than 5% of operations fail in 1 ms.  \r\n\r\nFor example, if you test at 24x and less than 5% of operations fail to complete in 1 ms, re-run the test at 48x, etc.  When the drive completes\r\nthe test at a particular speed with *more* than 5% of operations failing to complete in 1 ms (i.e., fails the test), then the drive is certified at the\r\nnext lower level where the drive DOES have fewer than 5% of errors in under 1 ms.\r\n\r\nIf your drive is testing well at higher loads, you may want to shorten the test time.  Running ACT for six hours\r\nwill give you a good idea whether your drive can pass ACT testing at a given traffic volume.  Before certifying your\r\ndrive at a given traffic level, we recommend a full 24-hour test.\r\n\r\nAs before, test a single drive first, and then test with multiple drives to make sure that the\r\nperformance scales linearly with your connector/controller.\r\n\r\n### Getting Started\r\n--------------------\r\n\r\n**Download the ACT package through git:**\r\n\r\n```\r\n$ git clone git@github.com:aerospike/act.git\r\n```\r\nThis creates an /act directory.  \r\n\r\nAlternately you can download the ZIP or TAR file from the links at the left.\r\nWhen you unpack/untar the file, it acreates an /aerospike-act-<version> directory.\r\n\r\n**Build the package.**\r\n\r\n```\r\n$ cd act    OR    cd /aerospike-act-<version>\r\n$ make\r\n$ make -f Makesalt\r\n```\r\n\r\nThis will create 2 binaries:\r\n\r\n* ***actprep***: This executable prepares a drive for ACT by writing zeroes on every sector of the disk and then filling it up with random data (salting). This simulates a normal production state.\r\n* ***act***: The ACT tool executable.\r\n\r\nThe root also contains a bash script called **runact** that runs actprep and act in a single process.\r\n\r\n### Running the ACT Certification Process \r\n---------------------\r\n\r\nTo certify your drive(s), first determine what certification test you will run, \r\nas described above in **Process for Certifying a Drive(s) for 3x Performance** or \r\n**Determining Expected Performance at Higher Throughput**.\r\n\r\nFor each certification test with ACT, you must perform the following steps:\r\n\r\n1. Prepare the drive(s) with actprep -- only the first time you test a drive(s)\r\n2. Create the config file for your test.\r\n3. Run the test, sending the results to a log file.\r\n4. Analyze log file output using the /latency_calc/act_latency.py script.\r\n5. Determine pass/fail for the test.\r\n\r\nThe details of these steps are described in detail below.\r\n\r\n**The tests destroy all data on the devices being tested!**\r\n\r\nWhen preparing devices and running tests, make sure the devices are\r\nspecified by name correctly.\r\n\r\nMake sure the test device is not mounted.\r\n\r\n\r\n#### 1. Prepare the Drives with actprep - First Time Only\r\n-------------\r\n\r\nThe first time you test a drive(s), you must\r\nprepare the drive(s) by first cleaning them (writing zeros everywhere) and\r\nthen \"salting\" them (writing random data everywhere) with actprep.\r\n\r\nactprep takes a device name as its only command-line parameter.  For\r\na typical 240GB SSD, actprep takes 30-60+ minutes to run. The time varies depending on the\r\ndrive and the capacity.\r\n\r\nIf you are testing multiple drives, you can run actprep on all of the drives in parallel. Preparing multiple drives\r\nin parallel does not take a lot more time than preparing a single drive, so this step should only take an hour or two.\r\n\r\nFor example, to clean and salt the device /dev/sdc: (over-provisioned using hdparm)\r\n```\r\n$ sudo ./actprep /dev/sdc &\r\n```\r\nIf you are using a RAID controller / over-provisioned using fdisk, make sure you specify the partition and not the raw\r\ndevice. If the raw device is used then ACT will wipe out the partition table and this will\r\ninvalidate the test.\r\n```\r\n$ sudo ./actprep /dev/sdc1 &\r\n```\r\n\r\n#### 2. Create a Configuration File\r\n-------------------------\r\n\r\nThe ACT package includes a Python script act_config_helper.py which helps you create a configuration file you can\r\nuse to run ACT. When you run this program it will: \r\n\r\n1. Ask you basic questions about the test you want to run\r\n2. Generate the correct config file, based on your answers\r\n\r\nTo run act_config_helper.py:\r\n```\r\n$ python act_config_helper.py\r\n```\r\nIf you are testing multiple drives, specify the drives and the desired traffic per drive per second, and the config\r\nfile will be created appropriately.\r\n\r\nAlternately you can create the config file manually by copying one of the sample config\r\nfiles in the /examples directory and modifying it, as described in the **ACT Configuration Reference** below.\r\n\r\n\r\n\r\n#### 3. Run the test\r\n---------\r\n\r\nFrom the ACT installation directory, run:\r\n```\r\n$ sudo ./act actconfig.txt > ouput.txt &\r\n```\r\nwhere:\r\n```\r\n* actconfig.txt - path/name for your config file name\r\n* output.txt    - path/name of your log file\r\n```\r\nIf running ACT from a remote terminal, it is best to run it as a background\r\nprocess, or within a \"screen\".  To verify that ACT is running, tail the output\r\ntext file with the -f option.\r\n\r\nNote that if the drive(s) being tested performs so badly that ACT's internal\r\ntransaction queues become extremely backed-up, ACT will halt before the\r\nconfigured test duration has elapsed.  ACT may also halt prematurely if it\r\nencounters unexpected drive I/O or system errors.\r\n\r\n\r\n#### 4. Analyze ACT Output\r\n--------------------\r\n\r\nRun /latency_calc/act_latency.py to process the ACT log file and tabulate data.  Note that you can run\r\nthe script when the test is not yet complete, and you will see the partial results.\r\n\r\nFor example:\r\n```\r\n$ ./act_latency.py -l output.txt\r\n```\r\n\r\nwhere:\r\n\r\n```\r\n -l <act output file name>   - required parameter that specifies the path/name of the log file generated by ACT\r\n -t <slice duration in seconds>  - optional parameter specifying slice length; default is 3600 sec (1 hour)\r\n```\r\n\r\nThe Python script analyzes the ACT output in time slices as specified, and displays\r\nlatency data and various verification intervals for each slice.  The script output will\r\nshow latencies both for end-to-end transactions (which include time spent on the\r\ntransaction queues) and for the device IO portion of transactions.\r\n\r\nThe example output below shows a 12-hour test (each slice is an hour).  The **trans** table\r\nshows transaction latency (end to end) and the **device** table at the right shows device latency.  So for \r\nexample, in the 5th hour, 1.68% of transactions failed to complete in under 1ms.\r\n\r\n```\r\n         trans                  device\r\n         %>(ms)                 %>(ms)\r\n slice        1      8     64        1      8     64\r\n -----   ------ ------ ------   ------ ------ ------\r\n     1     1.67   0.00   0.00     1.63   0.00   0.00\r\n     2     1.38   0.00   0.00     1.32   0.00   0.00\r\n     3     1.80   0.14   0.00     1.56   0.08   0.00\r\n     4     1.43   0.00   0.00     1.39   0.00   0.00\r\n     5     1.68   0.00   0.00     1.65   0.00   0.00\r\n     6     1.37   0.00   0.00     1.33   0.00   0.00\r\n     7     1.44   0.00   0.00     1.41   0.00   0.00\r\n     8     1.41   0.00   0.00     1.35   0.00   0.00\r\n     9     2.70   0.73   0.00     1.91   0.08   0.00\r\n    10     1.54   0.00   0.00     1.51   0.00   0.00\r\n    11     1.53   0.00   0.00     1.48   0.00   0.00\r\n    12     1.47   0.00   0.00     1.43   0.00   0.00\r\n -----   ------ ------ ------   ------ ------ ------\r\n   avg     1.62   0.07   0.00     1.50   0.01   0.00\r\n   max     2.70   0.73   0.00     1.91   0.08   0.00\r\n```\r\n\r\n#### 5. Evaluate Device(s) by the Standard Pass/Fail Criteria\r\n-------------------------\r\n\r\n##### Passing a Performance Test\r\nIn any one-hour period of an ACT performance test, we expect that:\r\n\r\n - fewer than 5% of transactions fail to complete in 1 ms\r\n - fewer than 1% of transactions fail to complete in 8 ms\r\n - fewer than 0.1% of transactions fail to complete in 64 ms\r\n\r\nThe **max** line of the output shows the highest values observed in any single slice (hour) of time\r\nand the values on the max line should not\r\nexceed the allowable error values specified above.  \r\n\r\nIn the example output above, we show only 12 hours of results, and the drive passes because the worst performance in any slice\r\nwas 2.7% of transactions failing to complete within 1 ms, 0.73% of transactions failing to complete in less\r\nthan 8 ms and no transactions failing to complete within 64 ms.\r\n\r\nA device(s) which does not exceed these error thresholds in 24 hours passes the load test.\r\n\r\n##### Passing a Stress Test \r\nWhen doing stress testing at a level ABOVE where the drive is certified, a device passes the test \r\nif ACT runs to completion, regardless of the number of errors.  \r\n\r\n## Tips and Tricks\r\n-----------------\r\nIf a drive is failing or there is a large discrepancy between the device and transaction\r\nlatencies, try increasing the number of threads in the config file by one or two (as described below).\r\n\r\nIf a drive has been used for some other purpose for a period of time before testing, then the\r\nspeed may have degraded and performance may be much poorer than a new drive of the same model.\r\n\r\n## ACT Configuration Reference\r\n----------------------\r\n\r\n#### Modifying the Config File Manually\r\n-------------\r\n\r\nFor ease of use, this package includes act_config_helper.py for creating config \r\nfiles. **Using act_config_helper.py is the recommended method for creating config files.**\r\n\r\nIf you are going to modify the config file manually, the package includes five example configuration files:\r\n\r\n* actconfig_1x.txt    - run a normal load test on one device\r\n* actconfig_3x.txt    - run a 3 times normal load test on one device\r\n* actconfig_6x.txt    - run a 6 times normal load test on one device\r\n* actconfig_12x.txt   - run a 12 times normal load test on one device\r\n* actconfig_24x.txt   - run a 24 times normal load test on one device\r\n* actconfig_1x_2d.txt - run a normal load test on two devices at a time\r\n* actconfig_1x_4d.txt - run a normal load test test on four devices at a time\r\n\r\nWhen modifying config files, you must be sure to set:\r\n\r\n1. the device name(s)\r\n2. the number of reads/writes to perform\r\n3. the number of large block operations to perform (large-block-ops-per-sec)\r\n\r\nFor example, to run a 48x test, you would modify\r\nthe actconfig_24x.txt file to specify the correct drive and the correct number of reads/writes per drive.  For a\r\ntest of 8 drives at 6x, you would modify the actconfig_1x_4d.txt file to specify all of your drives AND to specify the\r\nnumber of reads/writes to perform (6x rather than 1x).\r\n\r\nThe other fields in the configuration files should generally not be changed.  \r\n\r\n#### Format of Lines in the Config File\r\n-------------------\r\n\r\nAll fields use a \r\n```\r\nname-token: value\r\n```\r\nformat, and must be on a single line.\r\nField order in the file is unimportant.  To\r\nadd comments, add a line(s) that begin with '#'.  \r\n\r\n### Fields that you Must Change:\r\n\r\n**device-names**\r\nComma-separated list of device names (full path) to test.  For example:\r\n```\r\ndevice-names: /dev/sdb,/dev/sdc\r\n```\r\nMake sure the devices named are entered correctly.\r\n\r\n**num-queues**\r\nTotal number of queues.  If queue-per-device is set to yes, the num-queues field is ignored,\r\nsince in this case the number of queues is determined by the number of devices. if queue-per-device\r\nis set to no, you must specify the number of queues based on how many devices you are testing.\r\nWe recommend two queues per device.  Formula: 2 x number of devices.\r\n\r\n**read-reqs-per-sec**\r\nRead transactions/second to generate.  Note\r\nthat this is not per device, or per read transaction queue. For 3 times (3x)\r\nthe normal load for four drives, this value would be 3*4*2000 = 24000. Formula: n x number of drives x 2000\r\n\r\n**large-block-ops-per-sec**\r\nLarge-block write and large-block read operations per second.  Note that this is not per\r\ndevice. e.g. For 3 times (3x) the normal load for two drives, this value would be 3*2*23.5 = 141\r\n(rounded up). Formula: n x number of drives x 23.5\r\n\r\n### Fields that you will Sometimes Change:\r\n\r\n**threads-per-queue**\r\nNumber of threads per read\r\ntransaction queue. If a drive is failing and\r\nthere is a large discrepancy between transaction and device speeds from the ACT test\r\nyou can try increasing the number of threads.  Default is 8 threads/queue.\r\n\r\n**read-req-num-512-blocks**\r\nSize for each read\r\ntransaction, in 512-byte blocks, e.g. for 1.5-Kbyte reads (the default), this value would be 3.\r\n\r\n### Fields that you will Rarely or Never Change:\r\n\r\n**queue-per-device**\r\nFlag that determines ACT's internal read transaction queue setup -- yes means\r\neach device is read by a single dedicated read transaction queue, no means each\r\ndevice is read by all read transaction queues. If this field is left out, the default is no.\r\n\r\n**test-duration-sec**\r\nDuration of the entire test, in seconds.\r\nNote that it has to be a single number, e.g. use 86400, not 60*60*24.\r\nThe default is one day (24 hours).\r\n\r\n**report-interval-sec**\r\nInterval between generating observations,\r\nin seconds. This is the smallest granularity that you can analyze.  Default is 1 sec.  The\r\n/latency_calc/act_latency.py script aggregates these observations into slices, typically hour-long groups.\r\n\r\n**large-block-op-kbytes**\r\nSize written and read in each\r\nlarge-block write and large-block read operation respectively, in Kbytes.\r\n\r\n**use-valloc**\r\nFlag that determines ACT's memory allocation mechanism for read transaction\r\nbuffers -- yes means a system memory allocation call is used, no means dynamic\r\nstack allocation is used.  If this field is left out, the default is no.\r\n\r\n**num-write-buffers**\r\nNumber of different large blocks of random data we choose from when doing a\r\nlarge-block write operation -- 0 will cause all zeros to be written every time. \r\nIf this field is left out, the default is 0.  \r\n\r\n**scheduler-mode**\r\nMode in /sys/block/<device>/queue/scheduler for all the devices in\r\nthe test run -- noop means no special scheduling is done for device I/O\r\noperations, cfq means operations may be reordered to optimize for physical\r\nconstraints imposed by rotating disc drives (which likely means it hurts\r\nperformance for ssds).  If the field is left out, the default is noop.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}